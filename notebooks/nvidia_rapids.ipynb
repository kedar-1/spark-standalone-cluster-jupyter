{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "- **Compatibility with GPUs**:\n",
    "  - NVIDIA Turing Series (RTX 2000)\n",
    "  - NVIDIA Tesla T4\n",
    "  - NVIDIA A100\n",
    "  - NVIDIA Tesla V100\n",
    "- **CUDA Toolkit**: Version 11.0 or higher installed on your machine.\n",
    "- **Apache Spark**: Version 3.x.\n",
    "- **Java**: Version 8 or 11.\n",
    "- **RAPIDS Accelerator for Apache Spark**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "Using GPUs from the Maxwell series (architecture 5.2) will result in the following error:\n",
    "\n",
    "java.lang.RuntimeException: Device architecture 52 is unsupported. Minimum supported architecture: 70.\n",
    "\n",
    "Ensure that your GPU meets the minimum required architecture (7.0 or higher, rtx 2000+)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on RAPIDS Initialization\n",
    "\n",
    "If you see the following messages in the console:\n",
    "\n",
    "```bash\n",
    "24/12/29 20:43:16 WARN RapidsPluginUtils: RAPIDS Accelerator is enabled, to disable GPU support set spark.rapids.sql.enabled to false.\n",
    "24/12/29 20:43:16 INFO DriverPluginContainer: Initialized driver component for plugin com.nvidia.spark.SQLPlugin.\n",
    "```\n",
    "means that's all set up and ready to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Information\n",
    "\n",
    "The latest officially supported version of Apache Spark for RAPIDS is **3.5.3** [1].  \n",
    "During local deployment, version **3.5.2** was used.\n",
    "\n",
    "#### Issue with GPU Resource Allocation in Spark during Cross-Validation\n",
    "\n",
    "While setting up cross-validation in Spark, a problem occurred related to the improper allocation of GPU resources. Specifically, Spark was unable to properly configure access to the RTX 4000 series graphics card. This issue became apparent in the task manager, where the GPU resources were not being utilized, resulting in prolonged execution times.\n",
    "\n",
    "##### Encountered Problem:\n",
    "- **GPU Resources Not Allocated**: The RTX 4000 GPU resources were not allocated during the process.\n",
    "- **Slow Execution**: Due to the lack of GPU utilization, the process execution time was significantly extended.\n",
    "- **Configuration Suspected**: The issue is likely related to the configuration setting `.config(\"spark.rapids.sql.concurrentGpuTasks\", \"10\")`, which might have affected the GPU resource allocation.\n",
    "\n",
    "This issue may need further investigation into the GPU configuration and task concurrency settings in Spark.\n",
    "\n",
    "---\n",
    "\n",
    "#### Reference  \n",
    "[1] NVIDIA Spark RAPIDS Documentation: [https://nvidia.github.io/spark-rapids/docs/download.html](https://nvidia.github.io/spark-rapids/docs/download.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget https://archive.apache.org/dist/spark/spark-3.5.2/spark-3.5.2-bin-hadoop3.tgz\n",
    "!tar xf spark-3.5.2-bin-hadoop3.tgz\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://repo1.maven.org/maven2/com/nvidia/rapids-4-spark_2.12/24.10.1/rapids-4-spark_2.12-24.10.1-cuda11.jar\n",
    "!wget https://repo1.maven.org/maven2/ai/rapids/cudf/24.10.1/cudf-24.10.1-cuda11.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the paths\n",
    "os.environ['JAVA_HOME'] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ['SPARK_HOME'] = \"/home/test/and_t/spark-3.5.2-bin-hadoop3\" #Replace /home/test/and_t/ with the current working directory.\n",
    "\n",
    "import findspark\n",
    "findspark.init() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SPARK_RAPIDS_DIR'] = '/home/test/and_t'  # Replace with the actual RAPIDS Plugin directory\n",
    "os.environ['SPARK_RAPIDS_PLUGIN_JAR'] = f\"{os.environ['SPARK_RAPIDS_DIR']}/rapids-4-spark_2.12-24.10.1-cuda11.jar\"\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--jars \"  \\\n",
    "            + f\"{os.environ['SPARK_RAPIDS_DIR']}/rapids-4-spark_2.12-24.10.1-cuda11.jar\"  \\\n",
    "            + f\"{os.environ['SPARK_RAPIDS_DIR']}/cudf-24.10.1-cuda11.jar \" \\\n",
    "            + \" \" + \"--master local[*] pyspark-shell\"\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.environ['PYSPARK_SUBMIT_ARGS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkRAPIDS\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(spark.sparkContext.getConf().getAll())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkRAPIDS\") \\\n",
    "    .config(\"spark.plugins\", \"com.nvidia.spark.SQLPlugin\") \\\n",
    "    .config('spark.rapids.sql.enabled', 'true') \\\n",
    "    .config('spark.rapids.sql.incompatibleOps.enabled', 'true') \\\n",
    "    .config('spark.rapids.sql.format.csv.read.enabled', 'true') \\\n",
    "    .config(\"spark.rapids.sql.concurrentGpuTasks\", \"10\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"512m\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "    .config('spark.rapids.sql.format.csv.enabled', 'true') \\\n",
    "    .config(\"spark.rapids.sql.exec.CollectLimitExec\", \"true\")  \\\n",
    "    .config(\"spark.rapids.sql.explain\", \"OFF\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.addPyFile('/home/test/and_t/cudf-24.10.1-cuda11.jar')\n",
    "spark.sparkContext.addPyFile('/home/test/and_t/rapids-4-spark_2.12-24.10.1-cuda11.jar')\n",
    "\n",
    "print(spark.sparkContext.getConf().getAll())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing purpose we will use Credit card fraud detection dataset from Kaggle.\n",
    "https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/home/test/and_t/creditcard.csv\"\n",
    "fraud_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "fraud_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Select features and label\n",
    "feature_columns = [f\"V{i}\" for i in range(1, 29)] + [\"Amount\"]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "data = assembler.transform(fraud_df)\n",
    "data = data.select(\"features\", \"Class\")\n",
    "data = data.withColumnRenamed(\"Class\", \"label\")\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Define the logistic regression model\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "\n",
    "# Train the model\n",
    "model = lr.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "test_results = model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "roc_auc = evaluator.evaluate(test_results)\n",
    "\n",
    "print(f\"Test AUC: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Select features and label\n",
    "feature_columns = [f\"V{i}\" for i in range(1, 29)] + [\"Amount\"]\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "data = assembler.transform(data)\n",
    "data = data.select(\"features\", \"Class\")\n",
    "data = data.withColumnRenamed(\"Class\", \"label\")\n",
    "\n",
    "# Split the data into training and test sets\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Define the logistic regression model\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Create a parameter grid for hyperparameter tuning\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .addGrid(lr.maxIter, [10, 50, 100]) \\\n",
    "    .build()\n",
    "\n",
    "# Define the evaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "# Set up cross-validation\n",
    "crossval = CrossValidator(estimator=lr, \n",
    "                          estimatorParamMaps=param_grid, \n",
    "                          evaluator=evaluator, \n",
    "                          numFolds=5)\n",
    "\n",
    "# Train the model with cross-validation\n",
    "cv_model = crossval.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "test_results = cv_model.transform(test_data)\n",
    "\n",
    "# Evaluate the model\n",
    "roc_auc = evaluator.evaluate(test_results)\n",
    "\n",
    "print(f\"Test AUC: {roc_auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "and_t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
